{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e862afb937b169",
   "metadata": {},
   "source": [
    "本项目采用OpenRLHF框架\n",
    "<br>在开源的Pretrain Model基础上加以优化和魔改，实现以下RLHF三个部分的training：\n",
    "<br>1.Supervised Fine-tuning；\n",
    "<br>2.Reward Model；\n",
    "<br>3.Fine-tuned with Rl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88d31a60177a8f",
   "metadata": {},
   "source": [
    "# *SFT Training*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f56dace5b1757",
   "metadata": {},
   "source": [
    "本次选择 Qwen2-1.5B作为Pretrain Model， Alpaca-CoT@firefly作为数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba140ed44a52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set -x\n",
    "read -r -d '' training_commands <<EOF\n",
    "openrlhf.cli.train_sft \\\n",
    "  --max_len 2848 \\  # 最大序列长度\n",
    "  --dataset Qingyi/Alpaca-CoT@firefly \\\n",
    "  --input_key instruction \\  # 输入字段\n",
    "  --output_key output \\  # 输出字段\n",
    "  --train_batch_size 256 \\\n",
    "  --micro_train_batch_size 2 \\\n",
    "  --max_samples 500000 \\  # 最大训练样本数\n",
    "  --pretrain Qwen/Qwen2-1.5B \\  # pretrain model\n",
    "  --save_path ./checkpoint/qwen2-1.5b-firefly-sft \\\n",
    "  --save_steps -1 \\  # 多少步保存一个checkpoint，-1表示不保存\n",
    "  --logging_steps 1 \\\n",
    "  --eval_steps -1 \\  # 多少步evaluate一次，-1表示最后再evaluate一次\n",
    "  --zero_stage 2 \\  # deepspeed Zero阶段，1, 2, 3，推理，优化器状态和梯度部分到每张卡上\n",
    "  --max_epochs 1 \\  # 训练的epoch数量\n",
    "  --bf16 \\  # 是否开启BF16精度训练，默认启用\n",
    "  --flash_attn \\  # 是否启用flash attention，默认启用\n",
    "  --learning_rate 5e-6 \\\n",
    "  --load_checkpoint \\  # 是否load中间checkpoint\n",
    "  --gradient_checkpointing \\  # 是否启用梯度检查点技术\n",
    "  --use_wandb=[WANDB_TOKENS] \\  # wandb的User token\n",
    "  --wandb_project OpenRLHF \\\n",
    "  --wandb_run_name qwen2-1.5b-firefly-sft\n",
    "EOF\n",
    "\n",
    "# - wandb=[WANDB_TOKENS]\n",
    "# - packing_samples\n",
    "\n",
    "# 判断命令中第一个是不是slurm，如果不是启用deepspeed命令\n",
    "if [[ ${1} != \"slurm\" ]]; then\n",
    "    deepspeed --module $training_commands\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b3d7f5fa8ea83",
   "metadata": {},
   "source": [
    "为了节省训练时间，设置一个预加载model和dataset的脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3a126dd8ace3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 加载数据集的 firefly 文件夹\n",
    "dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", data_dir=\"firefly\")\n",
    "\n",
    "# 打印数据集的一些信息\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "model_name = \"Qwen/Qwen2-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 打印模型的一些信息\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb62670a7599dc",
   "metadata": {},
   "source": [
    "启动SFT训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e10478e4c5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup bash examples/scripts/train_sft_qwen1.8b.sh > output.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef6c708a8afca8",
   "metadata": {},
   "source": [
    "实验结果\n",
    "<br>Qwen2-1.5B，八卡3090一个epoch，5000000Samples，256 batch size，24H，loss_mean收敛在2.1左右"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0746cebdd1d3d8",
   "metadata": {},
   "source": "![](./images/loss_mean.png)"
  },
  {
   "cell_type": "markdown",
   "id": "ff8928f0f103c2b6",
   "metadata": {},
   "source": [
    "训练期间显存占用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02183b3615b89b0",
   "metadata": {},
   "source": "![训练期间显存占用](./images/Memory.png)"
  },
  {
   "cell_type": "markdown",
   "id": "2719592948c2891a",
   "metadata": {},
   "source": [
    "## *训练函数*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0cb6389b0f368",
   "metadata": {},
   "source": [
    "开启训练模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4482d24ace86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # 开启训练模式\n",
    "self.model.train()\n",
    "loss_mean = 0\n",
    "\n",
    "  # 从data loader 中读取训练dataset\n",
    "for inputs, labels, attention_masks, infos in self.train_dataloader:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf585044d7e5a0b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbab7c7504480a",
   "metadata": {},
   "source": [
    "加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2138bd8b4cc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels, attention_masks, infos in self.train_dataloader:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f281ac3d5edc6e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198c5a4feb44b50",
   "metadata": {},
   "source": [
    "GPU分配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe09a2dcb938e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.packing_samples:\n",
    "    inputs = inputs.to(torch.cuda.current_device())\n",
    "    attention_mask = attention_masks.to(torch.cuda.current_device())\n",
    "else:\n",
    "    inputs = inputs.to(torch.cuda.current_device()).squeeze(1)\n",
    "    attention_mask = attention_masks.to(torch.cuda.current_device()).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92103d9e200798",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f3a2131d9d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation\n",
    "\n",
    "if self.strategy.ring_attn_group is None:\n",
    "    output = self.model(inputs, attention_mask=attention_mask, return_output=True)\n",
    "else:\n",
    "    output = self.model(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        return_output=True,\n",
    "        ring_attn_group=self.strategy.ring_attn_group,\n",
    "        packed_seq_lens=infos[\"input_length\"]\n",
    "    )\n",
    "\n",
    "#Loss calculate\n",
    "labels = torch.where(\n",
    "    attention_mask.bool(),\n",
    "    inputs,\n",
    "    self.loss_fn.IGNORE_INDEX,\n",
    ")\n",
    "\n",
    "if self.aux_loss:\n",
    "    aux_loss = output.aux_loss\n",
    "else:\n",
    "    aux_loss = 0\n",
    "\n",
    "if not self.pretrain_mode:\n",
    "    if self.packing_samples:\n",
    "        index = 0\n",
    "        for input_length, source_len in zip(infos[\"input_length\"], prompt_id_lens):\n",
    "            labels[0][index : index + source_len] = self.loss_fn.IGNORE_INDEX\n",
    "            index += input_length\n",
    "    else:\n",
    "        for label, source_len in zip(labels, prompt_id_lens):\n",
    "            label[:source_len] = self.loss_fn.IGNORE_INDEX\n",
    "gpt_loss = self.loss_fn(output.logits, labels)\n",
    "loss = gpt_loss + aux_loss * self.args.aux_loss_coef\n",
    "\n",
    "#backward propagation\n",
    "\n",
    "self.strategy.backward(loss, self.model, self.optimizer)\n",
    "self.strategy.optimizer_step(self.optimizer, self.model, self.scheduler)\n",
    "\n",
    "loss_mean = loss_mean * 0.9 + 0.1 * gpt_loss.item()\n",
    "step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e6cc91c17905",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319dd8b71cc3e53",
   "metadata": {},
   "source": [
    "## SFT Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c6e542a9b3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT Language Model Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, ring_attn_group=None):\n",
    "        super().__init__()\n",
    "        self.IGNORE_INDEX = -100\n",
    "        # 本质上就是交叉熵Loss\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=self.IGNORE_INDEX)\n",
    "        self.ring_attn_group = ring_attn_group\n",
    "        if self.ring_attn_group:\n",
    "            self.ring_attn_rank = dist.get_rank(self.ring_attn_group)\n",
    "            self.ring_attn_world_size = dist.get_world_size(self.ring_attn_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd085f4ceb2a1c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        # RingAttention\n",
    "        if self.ring_attn_group is not None:\n",
    "            total_seq_len = labels.size(-1)\n",
    "            seq_len_per_process = total_seq_len // self.ring_attn_world_size\n",
    "            start_idx = self.ring_attn_rank * seq_len_per_process\n",
    "            end_idx = min(start_idx + seq_len_per_process, total_seq_len)\n",
    "            labels = labels[..., start_idx:end_idx]\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            if torch.all(shift_labels == self.IGNORE_INDEX):\n",
    "\n",
    "                loss = shift_logits.mean() * 0\n",
    "            else:\n",
    "                loss = self.loss(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                dist.all_reduce(loss, op=dist.ReduceOp.SUM, group=self.ring_attn_group)\n",
    "                loss = loss / self.ring_attn_world_size\n",
    "        else:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0063a4ef4006c1",
   "metadata": {},
   "source": [
    "此损失函数主要分为两种模式：\n",
    "<br>普通模式：直接计算交叉熵Loss\n",
    "<br>Ring Attention：分布式处理输入数据，让多GPU分别处理不同的序列部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad83ed81bb03ca",
   "metadata": {},
   "source": [
    "OpenRLHF简化版SFT Loss处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3761feab99b7a556",
   "metadata": {},
   "source": "![](./images/简化版SFTLoss.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3dd5d5c158c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.where(\n",
    "    attention_mask.bool(),\n",
    "    inputs,\n",
    "    self.loss_fn.IGNORE_INDEX,\n",
    ")\n",
    "for label, source_len in zip(labels, prompt_id_lens):\n",
    "    label[:source_len] = self.loss_fn.IGNORE_INDEX\n",
    "class GPTLMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT Language Model Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, ring_attn_group=None):\n",
    "        super().__init__()\n",
    "        self.IGNORE_INDEX = -100\n",
    "        # 本质就是交叉熵Loss\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=self.IGNORE_INDEX)\n",
    "def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "    # Labels和Logits对齐\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = self.loss(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278db2a11ea60286",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46279eef209a7f",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9318734e70629",
   "metadata": {},
   "source": [
    "原始的input和output（instruction/prompt和output）文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcffef5067cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"input\": \"User: 翻译成英文：\\n在国内，这些理论家的思想常常被嘲笑为中国异常主义的借口，和它对自己不喜欢的国际规则抗拒的托词。\\n\\nAssistant: \",\n",
    "  \"output\": \"Abroad these theorists’ ideas tend to be greeted with suspicion as excuses for China’s exceptionalism and its rejection of international rules it does not like.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c5cd99c1bc5e",
   "metadata": {},
   "source": [
    "input和output拼接后的token_ids，长度89，其中prompt_id_len为43，即前43个token是prompt部分。\n",
    "\n",
    "attention_mask的有效长度为74，也就是第74个token后面的部分是padding的，也可以看到他们的token_id都是一致的151643，这是一个special_token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a750d2a642ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input+output\n",
    "tensor([[  1474,     25,   10236,    123,    119,  102610,  12857,  105205,   28311,\n",
    "          913657,   5837, 700001, 101911,  45629, 105249,  100495,  99250,  108548,\n",
    "         112440,  104400,  62945, 109001,   9370, 111942,    3837,   33186,   99652,\n",
    "         169266, 105556,   99702,  278019,  71817, 115469,    3960,  137650,   55619,\n",
    "          82962,   67978,    8376,  380717,  43765,    448,  37041,\n",
    "          438,  54486,    5616,    748,  24364,   2142,    323,   1181,\n",
    "         36901,    315,    6489,   5601,    432,   1558,    537,   1075,    133,\n",
    "           220, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
    "        151643, 151643, 151643, 151643, 151643, 151643, 151643]])\n",
    "attention_mask\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2a617529be101",
   "metadata": {},
   "source": [
    "经过ignore_index操作之后可以得到abel，可以看到prompt和attention_mask部分已经被ignore了，这部分就不会参与后续的交叉熵计算了，这部分在 nn.CrossEntropyLoss() 中通过参数 ignore_index 控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd0aa164fa2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "label\n",
    "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "         82962, 67978, 8376, 380717, 43765, 448, 37041,\n",
    "         438, 54486, 5616, 748, 24364, 2142, 323, 1181,\n",
    "         36901, 315, 6489, 5601, 432, 1558, 537, 1075, 133,\n",
    "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62918235b79b92c",
   "metadata": {},
   "source": "![](./images/SFT对齐.png)"
  },
  {
   "cell_type": "markdown",
   "id": "90676c5259f3b8c0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2fe76e38a5192",
   "metadata": {},
   "source": [
    "## Packing Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00dcca3e8c90a9",
   "metadata": {},
   "source": [
    "在OpenRLHF的SFT训练代码train_sft.py中会在准备dataloader的时候，会有一个args.packing_samples的选项，这个决定了要不要对数据集中的数据做打包操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f28b27c391afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataloader\n",
    "train_dataloader = strategy.setup_dataloader(\n",
    "    train_dataset,\n",
    "    args.micro_train_batch_size,\n",
    "    True,\n",
    "    True,\n",
    "    train_dataset.packing_collate_fn if args.packing_samples else train_dataset.collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9cd6cf81998aa0",
   "metadata": {},
   "source": [
    "packing 的概念是将短输入合并/打包成单个输入，从而减少训练数据的数量。\n",
    "打包输入时，注意力应只集中在各个序列内部。如果不对attention mask做处理，那么就会如下图，不同的句子之间会做attention计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59b7088680253f",
   "metadata": {},
   "source": [
    "![](erro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc603091361b3851",
   "metadata": {},
   "source": [
    "正确处理应该是每个sample只保留自己内部的mask，如下图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4e67fe3948b62",
   "metadata": {},
   "source": "![](./images/Right.png)"
  },
  {
   "cell_type": "markdown",
   "id": "b0de26fb36c15e50",
   "metadata": {},
   "source": [
    "在[Packing Inputs Without Cross-Contamination Attention](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)中给出了packed_attention_mask的具体形式，就是属于第一个sample的mask全是1，第二个全是2，以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c561bdd2ea623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packing code\n",
    "def packing_collate_fn(self, item_list):\n",
    "    packed_input_ids = []\n",
    "    packed_attention_masks = []\n",
    "    prompt_ids_lens = []\n",
    "    infos = {\"input_length\": []}\n",
    "    index = 1\n",
    "\n",
    "    for prompt_ids_len, input_id, attention_mask, info in item_list:\n",
    "        # input token 拉平拼接到一起\n",
    "        packed_input_ids.append(input_id.flatten())\n",
    "\n",
    "        # 对于每一个sample生成一个大小相同的index的mask\n",
    "        packed_attention_masks.append(torch.full_like(input_id.flatten(), index))\n",
    "        prompt_ids_lens.append(prompt_ids_len)\n",
    "        infos[\"input_length\"].append(info[\"input_length\"])\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    packed_input_ids = torch.cat(packed_input_ids, dim=0).unsqueeze(0)\n",
    "    packed_attention_masks = torch.cat(packed_attention_masks, dim=0).unsqueeze(0)\n",
    "\n",
    "    return prompt_ids_lens, packed_input_ids, packed_attention_masks, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b720bc24ae4848",
   "metadata": {},
   "source": [
    "当pack两个sample的时候，第一个sample长度74，第二个sample长度89，产生的packed_attention_mask如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c31886ff51af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_attention_masks\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f9731d228be1e",
   "metadata": {},
   "source": [
    "在transformers.AutoModelForCausalLM类的forward()函数中传入这个attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c10510e05ac6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = self.model.forward(sequences, attention_mask=attention_mask, position_ids=position_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2be54a0c31789d",
   "metadata": {},
   "source": [
    "packing的好处是大幅度减少训练时间，通过将大量较短的sample packing成更少数量的sample，可以缩短训练时间。\n",
    "但是需要注意pack_length >= max_length，否则如果pack_length很大，打包后的数据量会很少，导致模型训练频率更新次数过少，结果变差。因此pack_length是一个超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2928a104524c84",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee4fe9cb8d16a8",
   "metadata": {},
   "source": [
    "# *Reward Model training*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2676b2b6b88c5045",
   "metadata": {},
   "source": [
    "选择了Qwen2.5-1.5B-Instruct作为训练Reward Model的基座，OpenRLHF/preference_dataset_mixture2_and_safe_pku作为偏好数据集进行reward model训练，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ff43160622914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练脚本\n",
    "set -x\n",
    "read -r -d '' training_commands <<EOF\n",
    "openrlhf.cli.train_rm \\\n",
    "  --save_path ./checkpoint/qwen2.5-1.5b-instruct-mixture2-rm \\\n",
    "  --save_steps -1 \\\n",
    "  --logging_steps 1 \\\n",
    "  --eval_steps -1 \\\n",
    "  --train_batch_size 256 \\\n",
    "  --micro_train_batch_size 1 \\\n",
    "  --pretrain Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "  --bf16 \\\n",
    "  --max_epochs 1 \\\n",
    "  --max_len 8192 \\\n",
    "  --zero_stage 3 \\  # 这里默认采用的是deepspeed的zero3策略，模型权重也会被切分到每张卡上\n",
    "  --learning_rate 9e-6 \\\n",
    "  --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n",
    "  --apply_chat_template \\  # 使用chat模板\n",
    "  --chosen_key chosen \\  # 偏好对中被偏好的字段\n",
    "  --rejected_key rejected \\  # 偏好对中不被偏好的字段\n",
    "  --flash_attn \\\n",
    "  --load_checkpoint \\\n",
    "  --gradient_checkpointing\n",
    "EOF\n",
    "\n",
    "# --use_wandb [WANDB_TOKENS] or True (use wandb Login command)\n",
    "# --packing_samples\n",
    "\n",
    "if [[ ${1} != \"slurm\" ]]; then\n",
    "    deepspeed --module $training_commands\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff16be90ec5619",
   "metadata": {},
   "source": [
    "偏好数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b26dde50e97c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  'rejected':\n",
    "  [\n",
    "    {'content': 'Part 1. Definition\\nYou are given a math word problem and you are supposed to apply division mathematical operator on the numbers embedded in the text to answer the following question and then only report the final numerical answer.\\nPart 2. Example\\nContext: Alex wants to split a collection of eggs into 4 groups. Alex has 8 eggs and 6 marbles. \\n\\nQuestion: How many eggs will each group have?\\nAnswer: 2\\nExplanation: Total eggs each group will get is 8/4 = 2\\nPart 3. Exercise\\nContext: 6 packs of dvds can be bought with 120 dollars. \\n\\nQuestion: How much does each pack cost?\\nAnswer:', 'role': 'user'},\n",
    "    {'content': 'Your task is to define the mathematical operation to be performed on the given numbers to solve the problem and provide a formula to calculate the answer. In this case, the task involves dividing the total amount of eggs by the number of groups, and your answer will be the number of eggs each group will have. I will provide you with the formula to solve the exercise, and if you have any questions, feel free to ask.', 'role': 'assistant'}\n",
    "  ],\n",
    "\n",
    "  'chosen':\n",
    "  [\n",
    "    {'content': 'Part 1. Definition\\nYou are given a math word problem and you are supposed to apply division mathematical operator on the numbers embedded in the text to answer the following question and then only report the final numerical answer.\\nPart 2. Example\\nContext: Alex wants to split a collection of eggs into 4 groups. Alex has 8 eggs and 6 marbles. \\n\\nQuestion: How many eggs will each group have?\\nAnswer: 2\\nExplanation: Total eggs each group will get is 8/4 = 2\\nPart 3. Exercise\\nContext: 6 packs of dvds can be bought with 120 dollars. \\n\\nQuestion: How much does each pack cost?\\nAnswer:', 'role': 'user'},\n",
    "    {'content': '6 packs x $120/pack = $720\\nExplanation: To find how much each pack costs, we need to divide the total cost by the number of packs. 120 dollars can buy 6 packs, so each pack costs $120/6 = $20.', 'role': 'assistant'}\n",
    "  ],\n",
    "\n",
    "  'rejected_score': 1.5,\n",
    "  'chosen_score': 3.75\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c62f5de9ebaa2",
   "metadata": {},
   "source": [
    "### 设置Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f922a029514758",
   "metadata": {},
   "source": [
    "主要是get_llm_for_sequence_regression()函数返回reward model，通过_get_reward_model()函数返回一个加了value head的model，然后对value head进行初始化。\n",
    "value head实际上是一个hidden_size -> 1的linear层，附加到选取的base model上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b5de5a5024ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_for_sequence_regression():\n",
    "    # main code\n",
    "    # Prioritize using the value_head_prefix in the model configuration.\n",
    "    value_head_prefix = getattr(config, \"value_head_prefix\", value_head_prefix)\n",
    "    logger.info(f\"set value_head_prefix to {value_head_prefix}\")\n",
    "    base_class = AutoModel._model_mapping[type(config)]\n",
    "    base_pretrained_class = base_class.__base__\n",
    "    cls_class = _get_reward_model(base_pretrained_class, base_class, value_head_prefix, packing_samples)\n",
    "\n",
    "    # NOTE: For reward model training only, initialize value_head manually\n",
    "    # because deepspeed.zero.Init() will not initialize them.\n",
    "    if init_value_head:\n",
    "        value_head = getattr(model, value_head_prefix)\n",
    "        value_head.weight.data.normal_(mean=0.0, std=1 / (config.hidden_size + 1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def _get_reward_model():\n",
    "    # main code\n",
    "    class RewardModel(base_pretrained_model):\n",
    "        supports_gradient_checkpointing = True\n",
    "\n",
    "        def __init__(self, config: AutoConfig):\n",
    "            super().__init__(config)\n",
    "            setattr(self, self.base_model_prefix, base_llm_model(config))\n",
    "            self.value_head_prefix = value_head_prefix\n",
    "            setattr(self, value_head_prefix, nn.Linear(config.hidden_size, 1, bias=False))\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            outputs = getattr(self, self.base_model_prefix)(\n",
    "                input_ids, attention_mask=attention_mask, position_ids=position_ids\n",
    "            )\n",
    "            last_hidden_states = outputs[\"last_hidden_state\"]\n",
    "            # 通过 value head 计算每个 token 的 reward 值\n",
    "            values = getattr(self, self.value_head_prefix)(last_hidden_states).squeeze(-1)\n",
    "            # 取句子的最后一个 token 作为最终 reward 值\n",
    "            eos_indices = attention_mask.size(1) - 1 - attention_mask.long().flip(1).argmax(dim=1, keepdim=True)\n",
    "            reward = values.gather(dim=1, index=eos_indices).squeeze(1)\n",
    "            return (reward, outputs) if return_output else reward\n",
    "\n",
    "    return RewardModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfda7086a0c067",
   "metadata": {},
   "source": [
    "#### Reward Model训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299be422fb6fc7a",
   "metadata": {},
   "source": [
    "整个 Reward Model 训练流程如下：\n",
    "\n",
    "<br>加载预训练模型（如 Qwen, LLaMA 等）。\n",
    "<br>在基础模型上附加 value head（一个从 hidden_size -> 1 的线性层）。\n",
    "<br>前向传播：处理输入文本；通过 value head 计算 token-wise reward；提取最终的句子 reward。\n",
    "<br>Loss计算（如排序损失、回归损失）。\n",
    "<br>优化器更新权重，迭代训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfc094292a9e1d7",
   "metadata": {},
   "source": "![](./images/Value%20head.png)"
  },
  {
   "cell_type": "markdown",
   "id": "75020fb7d21b00e7",
   "metadata": {},
   "source": [
    "取每一个token对应的last_hidden_states，过value head得到每一个token对应的value值，然后取句子末尾token对应的value值作为整个句子的reward值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9c5d2042954bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "\n",
    "DeepSpeedEngine(\n",
    "  (module): RewardModel(\n",
    "    (model): Qwen2Model(\n",
    "      (embed_tokens): Embedding(151936, 1536)\n",
    "      (layers): ModuleList(\n",
    "        (0-27): 28 x Qwen2DecoderLayer(\n",
    "          (self_attn): Qwen2FlashAttention2(\n",
    "            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
    "            (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
    "            (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
    "            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
    "            (rotary_emb): Qwen2RotaryEmbedding()\n",
    "          )\n",
    "          (mlp): Qwen2MLP(\n",
    "            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
    "            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
    "            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
    "            (act_fn): SiLU()\n",
    "          )\n",
    "          (input_layernorm): Qwen2RMSNorm((0,), eps=1e-06)\n",
    "          (post_attention_layernorm): Qwen2RMSNorm((0,), eps=1e-06)\n",
    "        )\n",
    "      )\n",
    "      (norm): Qwen2RMSNorm((0,), eps=1e-06)\n",
    "      (rotary_emb): Qwen2RotaryEmbedding()\n",
    "    )\n",
    "    (score): Linear(in_features=1536, out_features=1, bias=False)\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf613c9f545867",
   "metadata": {},
   "source": [
    "改造的Reward model结构如上，可以看到相比Qwen2Model类只多了一个用于score的线性层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344967ba2e0c06a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6f3decd47aff4",
   "metadata": {},
   "source": [
    "### 数据处理函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff766078b873e1",
   "metadata": {},
   "source": [
    "RewardDataset.collate_fn() 函数需要对 chosen、rejected 以及它们对应的 mask 做 left padding，这样方便对齐每个 sample 的最后位置，方便后续取出 reward。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f0f47c166b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(self, item_list):\n",
    "    chosen_ids = []\n",
    "    chosen_masks = []\n",
    "    reject_ids = []\n",
    "    rejects_masks = []\n",
    "    extras = []\n",
    "\n",
    "    for chosen_id, chosen_mask, reject_id, rejects_mask, extra in item_list:\n",
    "        chosen_ids.append(chosen_id)\n",
    "        chosen_masks.append(chosen_mask)\n",
    "        reject_ids.append(reject_id)\n",
    "        rejects_masks.append(rejects_mask)\n",
    "        extras.append(extra)\n",
    "\n",
    "    if self.is_dpo:\n",
    "        padding_side = \"right\"\n",
    "    else:\n",
    "        padding_side = \"left\"\n",
    "\n",
    "    chosen_ids = zero_pad_sequences(chosen_ids, side=padding_side, value=self.tokenizer.pad_token_id)\n",
    "    chosen_masks = zero_pad_sequences(chosen_masks, side=padding_side)\n",
    "    reject_ids = zero_pad_sequences(reject_ids, side=padding_side, value=self.tokenizer.pad_token_id)\n",
    "    rejects_masks = zero_pad_sequences(rejects_masks, side=padding_side)\n",
    "\n",
    "    return chosen_ids, chosen_masks, reject_ids, rejects_masks, extras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3d7d8cf7e8236",
   "metadata": {},
   "source": [
    "在数据的预处理中，对 prompt、chosen 和 rejected 进行处理，参数设置使用 chat_template，会调用模型的 tokenizer.apply_chat_template。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fedff11624a5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data() -> str:\n",
    "    if apply_chat_template:\n",
    "        if prompt_key:\n",
    "            prompt = apply_chat_template(data[prompt_key], tokenize=False, add_generation_prompt=True)\n",
    "            chosen = apply_chat_template(data[prompt_key] + data[chosen_key], tokenize=False)\n",
    "            rejected = apply_chat_template(data[prompt_key] + data[rejected_key], tokenize=False)\n",
    "        else:\n",
    "            prompt = \"\"\n",
    "            chosen = apply_chat_template(data[chosen_key], tokenize=False)\n",
    "            rejected = apply_chat_template(data[rejected_key], tokenize=False)\n",
    "\n",
    "        if is_dpo:\n",
    "            prompt = apply_chat_template(data[chosen_key][:-1], tokenize=False, add_generation_prompt=True)\n",
    "            chosen = chosen[len(prompt):]\n",
    "            rejected = rejected[len(prompt):]\n",
    "    else:\n",
    "        if prompt_key:\n",
    "            prompt = data[prompt_key]\n",
    "            if input_template:\n",
    "                prompt = input_template.format(prompt)\n",
    "        else:\n",
    "            prompt = \"\"\n",
    "\n",
    "        chosen = data[chosen_key]\n",
    "        rejected = data[rejected_key]\n",
    "\n",
    "    # margin Loss\n",
    "    margin = data[\"margin\"] if exist_and_not_none(data, \"margin\") else 0\n",
    "\n",
    "    return prompt, chosen, rejected, margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089b89935f20013",
   "metadata": {},
   "source": [
    "在 chosen 和 rejected 进入 reward model 之前，还需要做 concatenate 操作，这样可以避免对 chosen 和 rejected 单独跑两次 forward，提高训练效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96629be41cdedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_forward(self, model, chosen_ids, c_mask, reject_ids, r_mask):\n",
    "    \"\"\"\n",
    "    Run the given model on the given batch of inputs, concatenating the chosen and\n",
    "    rejected inputs together.\n",
    "    We do this to avoid doing two forward passes, because it's faster for FSDP.\n",
    "    \"\"\"\n",
    "    input_ids, att_masks = self.concatenated_inputs(chosen_ids, c_mask, reject_ids, r_mask)\n",
    "    all_values, output = model(input_ids, attention_mask=att_masks, return_output=True)\n",
    "    chosen_rewards = all_values[: chosen_ids.shape[0]]\n",
    "    rejected_rewards = all_values[chosen_ids.shape[0] :]\n",
    "    aux_loss = output.aux_loss if \"aux_loss\" in output else []\n",
    "    return chosen_rewards, rejected_rewards, aux_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aa56a725b50b1",
   "metadata": {},
   "source": [
    "#### RM训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5de39bb02afa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model.train()\n",
    "\n",
    "acc_mean = 0\n",
    "loss_mean = 0\n",
    "\n",
    "for data in self.train_dataloader:\n",
    "    chosen_ids, c_mask, reject_ids, r_mask, margin = data\n",
    "    chosen_ids = chosen_ids.squeeze(1).to(torch.cuda.current_device())\n",
    "    c_mask = c_mask.squeeze(1).to(torch.cuda.current_device())\n",
    "    reject_ids = reject_ids.squeeze(1).to(torch.cuda.current_device())\n",
    "    r_mask = r_mask.squeeze(1).to(torch.cuda.current_device())\n",
    "\n",
    "    chosen_reward, reject_reward, aux_loss = self.concatenated_forward(\n",
    "        self.model, chosen_ids, c_mask, reject_ids, r_mask\n",
    "    )\n",
    "\n",
    "    preference_loss = self.loss_fn(chosen_reward, reject_reward)\n",
    "\n",
    "    # mixtral\n",
    "    if not self.aux_loss:\n",
    "        aux_loss = 0\n",
    "\n",
    "    loss = preference_loss + aux_loss * self.args.aux_loss_coef\n",
    "    self.strategy.backward(loss, self.model, self.optimizer)\n",
    "    self.strategy.optimizer_step(self.optimizer, self.model, self.scheduler)\n",
    "\n",
    "    acc = (chosen_reward > reject_reward).float().mean().item()\n",
    "    acc_mean = acc_mean * 0.9 + 0.1 * acc\n",
    "    loss_mean = loss_mean * 0.9 + 0.1 * preference_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbca77b5b887fff",
   "metadata": {},
   "source": [
    "#### Reward Model Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0025d9ad881cf",
   "metadata": {},
   "source": "![](./images/RMLoss.png)"
  },
  {
   "cell_type": "markdown",
   "id": "4f5e219e38869163",
   "metadata": {},
   "source": [
    "有时会增加 margin 参数来控制 chosen 和 rejected 之间的距离，增大 margin 相当于增加训练难度，增强 chosen 和 rejected reward 的差值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0e6c16fb847c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWiseLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Pairwise Loss for Reward Model\n",
    "    \"\"\"\n",
    "    def forward(\n",
    "        self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor, margin: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        if margin is not None:\n",
    "            loss = -F.logsigmoid(chosen_reward - reject_reward - margin)\n",
    "        else:\n",
    "            loss = -F.logsigmoid(chosen_reward - reject_reward)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd472e6f7f3a14c1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c1a20926a9641",
   "metadata": {},
   "source": [
    "#   PPO训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eda3bde61a2b35",
   "metadata": {},
   "source": [
    "## PPO训练脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10748871418bf",
   "metadata": {},
   "source": [
    "这里选择了 Qwen2.5-1.5B-Instruction 作为训练PPO的基座模型，\n",
    "OpenRLHF/Llama-3-8b-rm-mixture 为 reward model，\n",
    "OpenRLHF/prompt-collection-v0.1 为 prompt 数据集进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16d8b9d09ee91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set -x\n",
    "read -r -d '' training_commands <<EOF\n",
    "openrlhf.cli.train_ppo \\\n",
    "  --pretrain Qwen/Qwen2.5-1.5B-Instruction \\  # 基座\n",
    "  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\  # reward model模型\n",
    "  --save_path ./checkpoint/llama-3-8b-rlhf \\\n",
    "  --save_steps -1 \\\n",
    "  --logging_steps 1 \\\n",
    "  --eval_steps -1 \\\n",
    "  --micro_train_batch_size 2 \\\n",
    "  --train_batch_size 128 \\\n",
    "  --micro_rollout_batch_size 4 \\  # PPO在进行RL时的生成数据的batch size\n",
    "  --rollout_batch_size 1024 \\  # PPO训练的总共某些prompt的batch size\n",
    "  --max_epochs 1 \\\n",
    "  --prompt_max_len 1024 \\\n",
    "  --generate_max_len 1024 \\\n",
    "  --zero_stage 2 \\\n",
    "  --bf16 \\\n",
    "  --actor_learning_rate 5e-7 \\\n",
    "  --critic_learning_rate 9e-6 \\\n",
    "  --init_kl_coef 0.01 \\\n",
    "  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n",
    "  --input_key context_messages \\\n",
    "  --apply_chat_template \\\n",
    "  --normalize_reward \\\n",
    "  --adam_offload \\\n",
    "  --flash_attn \\\n",
    "  --load_checkpoint \\\n",
    "  --gradient_checkpointing\n",
    "EOF\n",
    "\n",
    "# --packing_samples\n",
    "# --use_wandb [WANDB_TOKENS] or True (use wandb Login command)\n",
    "# --remote_rm_url http://localhost:5000/get_reward\n",
    "\n",
    "if [[ ${1} != \"slurm\" ]]; then\n",
    "    deepspeed --module $training_commands\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1eee2441f3dac",
   "metadata": {},
   "source": [
    "PPO 训练的流程如下：\n",
    "\n",
    "<br>加载 pretrain model 作为 Actor 进行训练。\n",
    "<br>加载 pretrain model 作为 Reference model，但 不训练。\n",
    "<br>加载 Reward model，同样 不训练，仅用于评估。\n",
    "<br>如果未指定 Critic model 的路径，则会自动使用 Reward model 作为 Critic 进行训练。\n",
    "s\n",
    "所"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666beb58cafd5499",
   "metadata": {},
   "source": [
    "所以PPO总共需要四个模型：Actor和Critic 需要训练，Reference和Reward不需要训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931a779f5f4a6f4",
   "metadata": {},
   "source": [
    "PPO目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293667283d32b5",
   "metadata": {},
   "source": "![](./images/PPO.png)"
  },
  {
   "cell_type": "markdown",
   "id": "c631ab3b65291c6c",
   "metadata": {},
   "source": [
    "### PPO训练主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748e3446ae1a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行了简化，只提取了主要部分\n",
    "\n",
    "\n",
    "# 从 prompts_dataloader 中采样 rollout_batch_size 个 prompts\n",
    "for rand_prompts in self.prompts_dataloader:\n",
    "    # 根据采样的 prompts 生成 experience，并加到 replay buffer 中\n",
    "    for i, experience in enumerate(\n",
    "        self.experience_maker.make_experience_list(rand_prompts, **self.generate_kwargs)\n",
    "    ):\n",
    "        self.replay_buffer.append(experience)\n",
    "\n",
    "    # 对优势值 adv 进行 normalize\n",
    "    self.replay_buffer.normalize(\"advantages\", self.strategy)\n",
    "\n",
    "    # PPO train\n",
    "    status = self.ppo_train(steps)\n",
    "\n",
    "    # 清除 replay buffer 中的数据\n",
    "    self.replay_buffer.clear()\n",
    "\n",
    "    if \"kl\" in status:\n",
    "        self.kl_ctl.update(status[\"kl\"], args.rollout_batch_size * args.n_samples_per_prompt)\n",
    "\n",
    "    steps = steps + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78134935c457678c",
   "metadata": {},
   "source": [
    "主要关注训练代码中的两个核心部分：Experience的生成和实际训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedfd253f6b7b6f9",
   "metadata": {},
   "source": [
    "#### make_experience_list 函数\n",
    "先对 prompt 用当前的模型策略生成 response。\n",
    "调用 make_experience 生成 experience，包括 reward 和 action_prob 等信息。\n",
    "计算带有 KL 约束的奖励值，并计算 experience 对应的 return 和 adv（优势函数）。\n",
    "这些 experience 最终将用于后续 PPO 训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c93caf830409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行了代码简化\n",
    "# 主要逻辑是根据 prompt 产生 response，并计算 reward 和 action_prob，并将这些数据存储起来作为 experience\n",
    "\n",
    "def make_experience_list(self, all_prompts: Union[str, List[str]], **generate_kwargs):\n",
    "    args = self.strategy.args\n",
    "    experiences = []\n",
    "\n",
    "    for samples in tqdm(self.generate_samples(all_prompts, **generate_kwargs)):\n",
    "        experiences.append(self.make_experience(samples))\n",
    "\n",
    "    # 简单处理，experience 中已经拥有 reward\n",
    "    experiences, rewards = self.process_experiences(experiences)\n",
    "\n",
    "    # 计算 return 和 advantages\n",
    "    for experience, reward in zip(experiences, rewards):\n",
    "        num_actions = experience.info[\"num_actions\"]\n",
    "        reward = compute_reward(\n",
    "            reward,\n",
    "            self.kl_ctl.value,\n",
    "            experience.kl,\n",
    "            action_mask=experience.action_mask,\n",
    "            num_actions=num_actions,\n",
    "            reward_clip_range=args.reward_clip_range,\n",
    "        )\n",
    "\n",
    "        experience.advantages, experience.returns = self.get_advantages_and_returns(\n",
    "            experience.values,\n",
    "            reward,\n",
    "            experience.action_mask,\n",
    "            generate_kwargs[\"gamma\"],\n",
    "            generate_kwargs[\"lambd\"],\n",
    "        )\n",
    "\n",
    "    return_sums = torch.tensor([each_reward.sum() for each_reward in reward])\n",
    "    experience.info[\"return\"] = return_sums\n",
    "\n",
    "    # 清理不必要的信息\n",
    "    experience.kl = None\n",
    "    del experience.info[\"num_actions\"]\n",
    "\n",
    "    return experiences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235fc7f485d8c7a",
   "metadata": {},
   "source": [
    "#### make_experience 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4f64eabf6af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experience(self, samples):\n",
    "    # 开启 eval 模式\n",
    "    self.actor.eval()\n",
    "    self.initial_model.eval()\n",
    "    self.reward_model.eval()\n",
    "    self.critic.eval()\n",
    "\n",
    "    # 从生成的 samples 中拿到生成的 response 序列 和 action mask\n",
    "    sequences = samples.sequences\n",
    "    attention_mask = samples.attention_mask\n",
    "    action_mask = samples.action_mask\n",
    "    num_actions = samples.num_actions\n",
    "\n",
    "    # Log probs (计算 Actor 模型在当前策略下的 log 概率)\n",
    "    action_log_probs = self.actor(sequences, num_actions, attention_mask)\n",
    "\n",
    "    # Init Log probs (计算 Reference/初始模型的 log 概率)\n",
    "    base_action_log_probs = self.initial_model(sequences, num_actions, attention_mask)\n",
    "\n",
    "    # Values (Critic 模型的价值预测)\n",
    "    value = self.critic(sequences, num_actions, attention_mask)\n",
    "\n",
    "    # Rewards (计算 Reward 模型的奖励得分)\n",
    "    r = self.reward_model(sequences, attention_mask)\n",
    "\n",
    "    # 当前策略和 Reference model 策略的 KL 散度\n",
    "    kl = action_log_probs - base_action_log_probs\n",
    "    if action_mask is not None:\n",
    "        kl = kl * action_mask\n",
    "\n",
    "    # reset model state (重置回训练模式)\n",
    "    self.actor.train()\n",
    "    self.critic.train()\n",
    "\n",
    "    return Experience(sequences, action_log_probs, value, attention_mask, action_mask, kl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ddf4ef45b0e59",
   "metadata": {},
   "source": [
    "#### Compute_Reward 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5f652a959128d",
   "metadata": {},
   "source": [
    "计算带有 KL 惩罚的奖励值，用于 PPO 训练中的策略约束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706944525e917c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(r, kl, action_mask):\n",
    "    kl_reward = - self.kl_coef * kl\n",
    "\n",
    "    eos_indices = action_mask.size(1) - 1 - action_mask.long().flip(dims=[1]).argmax(dim=1, keepdim=True)\n",
    "\n",
    "    last_reward = torch.zeros_like(kl).scatter_(dim=1, index=eos_indices, src=r.unsqueeze(2).to(kl.dtype))\n",
    "\n",
    "    return last_reward + kl_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82cf93063016f2",
   "metadata": {},
   "source": [
    "#### get_advantages_and_returns 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa06bda7b21677",
   "metadata": {},
   "source": [
    "计算广义优势估计（GAE），并返回最终的 return 值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d05b78498b440",
   "metadata": {},
   "source": "![](./images/GAE.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba0b7d8b201646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages_and_returns(values, rewards, gamma, lambd):\n",
    "    lastgae = 0\n",
    "    advantages_reversed = []\n",
    "    response_length = rewards.size(1)\n",
    "\n",
    "    # Mask invalid responses\n",
    "    if action_mask is not None:\n",
    "        values = action_mask * values\n",
    "        rewards = action_mask * rewards\n",
    "\n",
    "    for t in reversed(range(response_length)):  # reversed 从后往前算\n",
    "        nextvalues = values[:, t + 1] if t < response_length - 1 else 0.0\n",
    "        delta = rewards[:, t] + gamma * nextvalues - values[:, t]\n",
    "        lastgae = delta + gamma * lambd * lastgae\n",
    "        advantages_reversed.append(lastgae)\n",
    "\n",
    "    advantages = torch.stack(advantages_reversed[::-1], dim=1)\n",
    "    returns = advantages + values\n",
    "\n",
    "    return advantages.detach(), returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54791298cd19a7ff",
   "metadata": {},
   "source": [
    "### PPO_train 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba694d03aa9656",
   "metadata": {},
   "source": [
    "利用前面生成的experience数据进行模型训练。分为Actor和Critic两个部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5c07bbee274e5",
   "metadata": {},
   "source": [
    "#### training_step_actor 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0171cbf46f690f",
   "metadata": {},
   "source": [
    "PPO Actor Loss 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3b19af5b68b4",
   "metadata": {},
   "source": "![](./images/Actor%20Loss.png)"
  },
  {
   "cell_type": "markdown",
   "id": "cc7c63e1e5fb211b",
   "metadata": {},
   "source": [
    "这里加上了一个辅助的PTX Loss，即在pretrain的数据集上的预训练Loss来防止模型对原有的知识遗忘太多，也是一种稳定训练的方法，即PPO-ptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10510de2d8719972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step_actor(self, experience):\n",
    "    self.actor.train()\n",
    "\n",
    "    # 从 experience 中提取数据\n",
    "    sequences = experience.sequences\n",
    "    old_action_log_probs = experience.action_log_probs\n",
    "    advantages = experience.advantages\n",
    "    num_actions = experience.action_mask.size(1)\n",
    "    packed_seq_lens = None\n",
    "    attention_mask = experience.attention_mask\n",
    "\n",
    "    # 计算新策略的 action log probabilities\n",
    "    action_log_probs, output = self.actor(\n",
    "        sequences,\n",
    "        num_actions,\n",
    "        attention_mask=attention_mask,\n",
    "        return_output=True,\n",
    "        packed_seq_lens=packed_seq_lens,\n",
    "    )\n",
    "\n",
    "    # 计算 PPO Actor Loss\n",
    "    actor_loss = self.actor_loss_fn(\n",
    "        action_log_probs,\n",
    "        old_action_log_probs,\n",
    "        advantages,\n",
    "        action_mask=experience.action_mask,\n",
    "    )\n",
    "\n",
    "    self.strategy.backward(actor_loss, self.actor, self.actor_optim)\n",
    "\n",
    "    # 预训练损失（PTX loss）：在 pretrain 数据上添加额外的损失项\n",
    "    if self.pretrain_dataloader is not None:\n",
    "        data = next(self.pretrain_dataloader)\n",
    "        inputs = data[1].squeeze(1).to(torch.cuda.current_device())\n",
    "        attention_mask = data[2].squeeze(1).to(torch.cuda.current_device())\n",
    "\n",
    "        # 设置目标标签\n",
    "        label = torch.where(\n",
    "            attention_mask.bool(),\n",
    "            inputs,\n",
    "            self.ptx_loss_fn.IGNORE_INDEX,\n",
    "        )\n",
    "\n",
    "        # 计算预训练损失\n",
    "        output = self.actor(inputs, attention_mask=attention_mask, return_output=True)\n",
    "        ptx_log_probs = output[\"logits\"]\n",
    "\n",
    "        # 计算 PTX Loss\n",
    "        ptx_loss = self.ptx_loss_fn(ptx_log_probs, label)\n",
    "        self.strategy.backward(self.ptx_coef * ptx_loss, self.actor, self.actor_optim)\n",
    "\n",
    "    self.strategy.optimizer_step(self.actor_optim, self.actor, self.actor_scheduler, name=\"actor\")\n",
    "\n",
    "    return status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776ea3145009c40",
   "metadata": {},
   "source": [
    "#### training_step_critic 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8924ec264c4d2ed",
   "metadata": {},
   "source": [
    "Value Loss是简单的MSEloss，计算value和return直接的MSEloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583153274ef1b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step_critic(self, experience):\n",
    "    self.critic.train()\n",
    "\n",
    "    sequences = experience.sequences\n",
    "    old_values = experience.values\n",
    "    returns = experience.returns\n",
    "    num_actions = experience.action_mask.size(1)\n",
    "    packed_seq_lens = None\n",
    "    attention_mask = experience.attention_mask\n",
    "\n",
    "    # 计算当前 Critic 预测的值\n",
    "    values, output = self.critic(\n",
    "        sequences,\n",
    "        num_actions=num_actions,\n",
    "        attention_mask=attention_mask,\n",
    "        return_output=True,\n",
    "        packed_seq_lens=packed_seq_lens,\n",
    "    )\n",
    "\n",
    "    # 计算 Critic Loss\n",
    "    critic_loss = self.critic_loss_fn(\n",
    "        values,\n",
    "        old_values,\n",
    "        returns,\n",
    "        action_mask=experience.action_mask,\n",
    "    )\n",
    "\n",
    "    # 反向传播计算梯度\n",
    "    self.strategy.backward(critic_loss, self.critic, self.critic_optim)\n",
    "\n",
    "    # 更新 Critic 参数\n",
    "    self.strategy.optimizer_step(self.critic_optim, self.critic, self.critic_scheduler, name=\"critic\")\n",
    "\n",
    "    return status\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
